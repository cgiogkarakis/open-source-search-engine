# All <, >, " and # characters that are values for a field contained herein
# must be represented as &lt;, &gt;, &#34; and &#035; respectively.

# When enabled the spider adds pages to your index. 
<spideringEnabled>1</>

# make each spider wait this many milliseconds before getting the ip and
# downloading the page.
<spiderDelayInMilliseconds>0</>

# What is the maximum number of web pages the spider is allowed to download
# simultaneously?
<maxSpiders>7</>

# If this is enabled others can add web pages to your index via the add url
# page.
<addUrlEnabled>1</>

# Do a tight merge on indexdb and datedb at this time every day. This is
# expressed in MINUTES past midnight UTC. UTC is 5 hours ahead of EST and 7
# hours ahead of MST. Leave this as -1 to NOT perform a daily merge. To merge
# at midnight EST use 60*5=300 and midnight MST use 60*7=420.
<dailyMergeTime>-1</>

# Comma separated list of days to merge on. Use 0 for Sunday, 1 for Monday,
# ... 6 for Saturday. Leaving this parmaeter empty or without any numbers will
# make the daily merge happen every day
<dailyMergeDays><![CDATA[0]]></>

# When the daily merge was last kicked off. Expressed in UTC in seconds since
# the epoch.
<dailyMergeLastStarted>-1</>

# Index documents for generating results sorted by date or constrained by date
# range. Only documents indexed while this is enabled will be returned for
# date-related searches.
<useDatedb>0</>

# Do not index pubdates into datedb that are more than this many days old. Use
# -1 for no limit. A value of zero essentially turns off datedb. Pre-existing
# pubdates in datedb that fail to meet this constraint WILL BE COMPLETELY
# ERASED when datedb is merged.
<ageCutoffForDatedb>-1</>

# Default timezone to use when none specified on parsed time.  Use offset from
# GMT, i.e 0400 (AMT) or -0700 (MST)
<datedbDefaultTimezone>0</>

# If this is true, users will have to pass a simple Turing test to add a url.
# This prevents automated url submission.
<turingTestEnabled>0</>

# If this is false, the spider will not harvest links from web pages it
# visits. Links that it does harvest will be attempted to be indexed at a
# later time. 
<spiderLinks>1</>

# If this is true the spider will only harvest links to pages that are
# contained on the same host as the page that is being spidered. Example: When
# spidering a page from www.gigablast.com, only links to pages that are from
# www.gigablast.com would be harvested, if this switch were enabled. This
# allows you to seed the spider with URLs from a specific set of hosts and
# ensure that only links to pages that are from those hosts are harvested.
<onlySpiderLinksFromSameHost>0</>

# If less than this many days have elapsed since the last time we added the
# outlinks to spiderdb, do not re-add them to spiderdb. Saves resources.
<doNotReaddOldOutlinksMoreThanThisManyDays>30.000</>

# Do searches for queries in this hosts part of the query log.
<scrapingEnabledProcog>0</>

# Perform random searches on googles news search engine to add sites with
# ingoogle tags into tagdb.
<scrapingEnabledWeb>0</>

# Perform random searches on googles news search engine to add sites with news
# and goognews and ingoogle tags into tagdb.
<scrapingEnabledNews>0</>

# Perform random searches on googles news search engine to add sites with
# blogs and googblogs and ingoogle tags into tagdb.
<scrapingEnabledBlogs>0</>

# Add the "sitepathdepth" to tagdb if a hostname is determined to have
# subsites at a particular depth.
<subsiteDetectionEnabled>0</>

# When enabled, the spider will discard web pages which are identical to other
# web pages that are already in the index AND that are from the same hostname.
# An example of a hostname is www1.ibm.com. However, root urls, urls that have
# no path, are never discarded. 
<dedupingEnabled>0</>

# When enabled, the spider will discard web pages which, when a www is
# prepended to the page's url, result in a url already in the index.
<dedupingEnabledForWww>1</>

# Detect and do not index pages which have a 200 status code, but are likely
# to be error pages.
<detectCustomErrorPages>1</>

# Should pages be removed from the index if they are no longer accessible on
# the web?
<delete404s>1</>

# Should documents be deleted from the index if they have been retried them
# enough times and the last received error is a time out? If your internet
# connection is flaky you may say no here to ensure you do not lose important
# docs.
<deleteTimedOutDocs>0</>

# If this is true, the spider, when a url redirects to a "simpler" url, will
# add that simpler url into the spider queue and abandon the spidering of the
# current url.
<useSimplifiedRedirects>1</>

# If this is true, the spider, when updating a web page that is already in the
# index, will not even download the whole page if it hasn't been updated since
# the last time Gigablast spidered it. This is primarily a bandwidth saving
# feature. It relies on the remote webserver's returned Last-Modified-Since
# field being accurate.
<useIfModifiedSince>0</>

# If this is true, the spider, when checking the page if it has changed enough
# to reindex or update the published date, it will build the vector only from
# the content located on that page.
<buildSimilarityVectorFromContentOnly>1</>

# This requires build similarity from content only to be on.  This indexes the
# publish date (only if the content has changed enough) to be between the last
# two spider dates.
<useContentSimilarityToIndexPublishDate>1</>

# This requires build similarity from content only and use content similarity
# to index publish date to be on.  This percentage is the maximum similarity
# that can exist between an old document and new before the publish date will
# be updated.
<maxPercentageSimilarToUpdatePublishDate>80</>

# If this is true Gigablast will respect the robots.txt convention.
<useRobotstxt>1</>

# If this is true and the spider finds lewd words in the hostname of a url it
# will throw that url away. It will also throw away urls that have 5 or more
# hyphens in their hostname.
<doUrlSpornChecking>0</>

# Hours to wait after trying to add an unspiderable url to spiderdb again.
<hoursBeforeAddingUnspiderableUrlToSpiderdb>24</>

# If this is true then new documents will be removed from the index if the
# quota for their domain has been breeched.
<enforceDomainQuotasOnNewDocs>0</>

# If this is true then indexed documents will be removed from the index if the
# quota for their domain has been breeched.
<enforceDomainQuotasOnIndexedDocs>0</>

# Does not use approximations so will do more disk seeks and may impact
# indexing performance significantly.
<useExactQuotas>0</>

# If this is true then only the root indexb file is searched for linkers.
# Saves on disk seeks, but may use older versions of indexed web pages.
<restrictIndexdbForSpidering>0</>

# Merge is triggered when this many spiderdb data files are on disk.
<spiderdbMinFilesNeededToTriggerToMerge>8</>

# Merge is triggered when this many clusterdb data files are on disk.
<clusterdbMinFilesNeededToTriggerToMerge>2</>

# Merge is triggered when this many linkdb data files are on disk.
<linkdbMinFilesNeededToTriggerToMerge>4</>

# Merge is triggered when this many titledb data files are on disk.
<titledbMinFilesNeededToTriggerToMerge>6</>

# Merge is triggered when this many posdb data files are on disk.
<posdbMinFilesNeededToTriggerToMerge>6</>

# Rather than downloading the content again when indexing old urls, use the
# stored content. Useful for reindexing documents under a different ruleset or
# for rebuilding an index. You usually should turn off the 'use robots.txt'
# switch. And turn on the 'use old ips' and 'recycle link votes' switches for
# speed. If rebuilding an index then you should turn off the 'only index
# changes' switches.
<recycleContent>0</>

# If this is true Gigablast will index hyper-link text and use hyper-link
# structures to boost the quality of indexed documents.
<enableLinkVoting>1</>

# If this is true, do not allow spammy inlinks to vote. This check is too
# aggressive for some collections, i.e.  it does not allow pages with cgi in
# their urls to vote.
<doLinkSpamChecking>1</>

# Use the links: termlists instead of link:. Also allows pages linking from
# the same domain or IP to all count as a single link from a different IP.
# This is also required for incorporating RSS and Atom feed information when
# indexing a document.
<useNewLinkAlgo>1</>

# How often should Gigablast recompute the link info for a url. Also applies
# to getting the quality of a site or root url, which is based on the link
# info. In days. Can use decimals. 0 means to update the link info every time
# the url's content is re-indexed. If the content is not reindexed because it
# is unchanged then the link info will not be updated. When getting the link
# info or quality of the root url from an external cluster, Gigablast will
# tell the external cluster to recompute it if its age is this or higher.
<updateLinkInfoFrequency>60.000</>

# If this is true Gigablast will only allow one vote per the top 2 significant
# bytes of the IP address. Otherwise, multiple pages from the same top IP can
# contribute to the link text and link-based quality ratings of a particular
# URL. Furthermore, no votes will be accepted from IPs that have the same top
# 2 significant bytes as the IP of the page being indexed.
<restrictLinkVotingByIp>1</>

# If this is true Gigablast will index the plain text surrounding the
# hyper-link text. The score will be x times that of the hyper-link text,
# where x is the scalar below.
<indexInlinkNeighborhoods>1</>

# Sometimes you want the spiders to use the tagdb of another collection, like
# the <i>main</i> collection. If this is empty it defaults to the current
# collection.
<tagdbCollectionName><![CDATA[]]></>

# Spiders will look to see if the current page is in catdb.  If it is, all
# Directory information for that page will be indexed with it.
<catdbLookupsEnabled>1</>

# Rather than requesting new info from DMOZ, like titles and topic ids, grab
# it from old record. Increases performance if you are seeing a lot of
# "getting catdb record" entries in the spider queues.
<recycleCatdbInfo>0</>

# If this is 'NO' then pages that are in catdb, but banned from tagdb or the
# url filters page, can not be banned.
<allowBanningOfPagesInCatdb>0</>

# Ignore and skip spider errors if the spidered site is found in Catdb (DMOZ).
<overrideSpiderErrorsForCatdb>1</>

# If this is disabled the spider will not allow any docs from the gb2312
# charset into the index.
<allowAsianDocs>1</>

# If this is disabled the spider will not allow any docs which contain adult
# content into the index (overides tagdb).
<allowAdultDocs>1</>

# If this is disabled the spider will not allow any xml into the index.
<allowXmlDocs>0</>

# If this is eabled the spider will not allow any docs which are determined to
# be serps.
<doSerpDetection>1</>

# If this is disabled and the proxy IP below is not zero then Gigablast will
# assume all spidered URLs have an IP address of 1.2.3.4.
<doIPLookup>1</>

# Should the stored IP of documents we are reindexing be used? Useful for
# pages banned by IP address and then reindexed with the reindexer tool.
<useOldIPs>0</>

# Remove banned pages from the index. Pages can be banned using tagdb or the
# Url Filters table.
<removeBannedPages>1</>

# If this is false then the filter will not be used on html or text pages.
<applyFilterToTextPages>0</>

# If this is true, spiders will read HTTPS pages using SSL Protocols.
<allowHTTPSPagesUsingSSL>0</>

# If an item on a page has an RSS feed link, add the RSS link to the spider
# queue and index the RSS pages instead of the current page.
<followRSSLinks>0</>

# Only index pages that were linked to by an RSS feed. Follow RSS Links must
# be enabled (above).
<onlyIndexArticlesFromRSSFeeds>0</>

# Maximum number of urls that can be submitted via the addurl interface, per
# IP domain, per 24 hour period. A value less than or equal to zero implies no
# limit.
<maxAddUrls>100</>

# Program to spawn to filter all HTTP replies the spider receives. Leave blank
# for none.
<filterName><![CDATA[]]></>

# Kill filter shell after this many seconds. Assume it stalled permanently.
<filterTimeout>40</>

# Retrieve pages from the proxy at this IP address.
<proxyIp>0.0.0.0</>

# Retrieve pages from the proxy on this port.
<proxyPort>0</>

# How many second to cache a robots.txt file for. 86400 is 1 day. 0 means
# Gigablast will not read from the cache at all and will download the
# robots.txt before every page if robots.txt use is enabled above. However, if
# this is 0 then Gigablast will still store robots.txt files into the cache.
<maxRobotstxtCacheAge>86400</>

# Only spider URLs scheduled to be spidered at this time or after. In UTC.
<spiderStartTime>15 Jan 1970 21:00 UTC</>

# Only spider URLs scheduled to be spidered at this time or before. If "use
# current time" is true then the current local time is used for this value
# instead. in UTC.
<spiderEndTime>15 Jan 2010 21:00 UTC</>

# Use the current time as the spider end time?
<useCurrentTime>1</>

# How many times should the spider be allowed to fail to download a particular
# web page before it gives up? Failure may result from temporary loss of
# internet connectivity on the remote end, dns or routing problems.
<numberRetriesPerUrl>1</>

# Keep this pretty high so that we get problem urls out of the index fast,
# otherwise, you might be waiting months for another retry. Use
# <i>undefined</i> to indicate no change in the priority of the url.
<priorityOfUrlsBeingRetried>-1</>

# Weight title this much more or less. This units are percentage. A 100 means
# to not give the title any special weight. Generally, though, you want to
# give it significantly more weight than that, so 2400 is the default.
<titleWeight>4600</>

# Weight terms in header tags by this much more or less. This units are
# percentage. A 100 means to not give the header any special weight.
# Generally, though, you want to give it significantly more weight than that,
# so 600 is the default.
<headerWeight>600</>

# Weight text in url path this much more. The units are percentage. A 100
# means to not give any special weight. Generally, though, you want to give it
# significantly more weight than that, so 600 is the default.
<urlPathWordWeight>1600</>

# Weight text in the incoming external link text this much more. The units are
# percentage. It already receives a decent amount of weight naturally.
<externalLinkTextWeight>600</>

# Weight text in the incoming internal link text this much more. The units are
# percentage. It already receives a decent amount of weight naturally.
<internalLinkTextWeight>200</>

# Weight concepts this much more. The units are percentage. It already
# receives a decent amount of weight naturally. AKA: surrounding text boost.
<conceptWeight>50</>

# If this is true Gigablast will only search the root index file for docIds.
# Saves on disk seeks, but may use older versions of indexed web pages.
<restrictIndexdbForQueries>0</>

# Like above, but specifically for XML feeds.
<restrictIndexdbForXmlFeed>1</>

# Should we read search results from the cache? Set to false to fix dmoz bug.
<readFromCacheByDefault>0</>

# Should search results be site clustered by default?
<siteClusterByDefault>1</>

# Hide all clustered results instead of displaying two results from each site.
<hideAllClusteredResults>0</>

# Should duplicate search results be removed by default?
<dedupResultsByDefault>1</>

# Should we dedup URLs with case insensitivity? This is mainly to correct
# duplicate wiki pages.
<dedupURLs>0</>

# Use language specific pages for home, etc.
<useVhostLanguageDetection>1</>

# Use Language weights to sort query results. This will give results of a
# similar language a higher priority.
<useLanguageWeights>1</>

# Default language for post query rerank. This should only be used on limited
# collections. Value should be any language abbreviation, for example "en" for
# English.
<sortLanguagePreference><![CDATA[en_US]]></>

# Default country for post query rerank. This should only be used on limited
# collections. Value should be any country code abbreviation, for example "us"
# for United States.
<sortCountryPreference><![CDATA[us]]></>

# How many search results should we scan for post query demotion? 0 disables
# all post query reranking. 
<docsToCheckForPostQueryDemotion>0</>

# Demotion factor of non-relevant languages.  Score will be penalized by this
# factor as a percent if it's language is foreign. A safe value is probably
# anywhere from 0.5 to 1. 
<demotionForForeignLanguages>0.999</>

# Demotion factor for unknown languages. Page's score will be penalized by
# this factor as a percent if it's language is not known. A safe value is 0,
# as these pages will be reranked by country (see below). 0 means no demotion.
<demotionForUnknownLanguages>0.000</>

# Demotion for pages where the country of the page writes in the same language
# as the country of the query. If query language is the same as the language
# of the page, then if a language written in the country of the page matches a
# language written by the country of the query, then page's score will be
# demoted by this factor as a percent. A safe range is between 0.5 and 1. 
<demotionForPagesWhereTheCountryOfThePageWritesInTheSameLanguageAsTheCountryOfTheQuery>0.980</>

# Demotion factor for query terms or gigabits in a result's url. Score will be
# penalized by this factor times the number of query terms or gigabits in the
# url divided by the max value below such that fewer query terms or gigabits
# in the url causes the result to be demoted more heavily, depending on the
# factor. Higher factors demote more per query term or gigabit in the page's
# url. Generally, a page may not be demoted more than this factor as a
# percent. Also, how it is demoted is dependant on the max value. For example,
# a factor of 0.2 will demote the page 20% if it has no query terms or
# gigabits in its url. And if the max value is 10, then a page with 5 query
# terms or gigabits in its url will be demoted 10%; and 10 or more query terms
# or gigabits in the url will not be demoted at all. 0 means no demotion. A
# safe range is from 0 to 0.35. 
<demotionForQueryTermsOrGigabitsInUrl>0.000</>

# Max number of query terms or gigabits in a url. Pages with a number of query
# terms or gigabits in their urls greater than or equal to this value will not
# be demoted. This controls the range of values expected to represent the
# number of query terms or gigabits in a url. It should be set to or near the
# estimated max number of query terms or topics that can be in a url. Setting
# to a lower value increases the penalty per query term or gigabit that is not
# in a url, but decreases the range of values that will be demoted.
<maxValueForPagesWithQueryTermsOrGigabitsInUrl>10</>

# Demotion factor for pages that are not high quality. Score is penalized by
# this number as a percent times level of quality. A pqge will be demoted by
# the formula (max quality - page's quality) * this factor / the max value
# given below. Generally, a page will not be demoted more than this factor as
# a percent. 0 means no demotion. A safe range is between 0 to 1. 
<demotionForPagesThatAreNotHighQuality>0.000</>

# Max page quality. Pages with a quality level equal to or higher than this
# value will not be demoted. 
<maxValueForPagesThatAreNotHighQuality>100</>

# Demotion factor each path in the url. Score will be demoted by this factor
# as a percent multiplied by the number of paths in the url divided by the max
# value below. Generally, the page will not be demoted more than this value as
# a percent. 0 means no demotion. A safe range is from 0 to 0.75. 
<demotionForPagesThatAreNotRootOrHaveManyPathsInTheUrl>0.000</>

# Max number of paths in a url. This should be set to a value representing a
# very high number of paths for a url. Lower values increase the difference
# between how much each additional path demotes. 
<maxValueForPagesThatHaveManyPathsInTheUrl>16</>

# Demotion factor for pages that do not have a catid. Score will be penalized
# by this factor as a percent. A safe range is from 0 to 0.2. 
<demotionForPagesThatDoNotHaveACatid>0.000</>

# Demotion factor for pages where smallest catid has a lot of super topics.
# Page will be penalized by the number of super topics multiplied by this
# factor divided by the max value given below. Generally, the page will not be
# demoted more than this factor as a percent. Note: pages with no catid are
# demoted by this factor as a percent so as not to penalize pages with a
# catid. 0 means no demotion. A safe range is between 0 and 0.25. 
<demotionForPagesWhereSmallestCatidHasALotOfSuperTopics>0.000</>

# Max number of super topics. Pages whose smallest catid that has more super
# topics than this will be demoted by the maximum amount given by the factor
# above as a percent. This should be set to a value representing a very high
# number of super topics for a category id. Lower values increase the
# difference between how much each additional path demotes. 
<maxValueForPagesWhereSmallestCatidHasALotOfSuperTopics>11</>

# Demotion factor for larger pages. Page will be penalized by its size times
# this factor divided by the max page size below. Generally, a page will not
# be demoted more than this factor as a percent. 0 means no demotion. A safe
# range is between 0 and 0.25. 
<demotionForLargerPages>0.000</>

# Max page size. Pages with a size greater than or equal to this will be
# demoted by the max amount (the factor above as a percent). 
<maxValueForLargerPages>524288</>

# Demotion factor for non-location specific queries with a location specific
# title. Pages which contain a location in their title which is not in the
# query or the gigabits will be demoted by their population multiplied by this
# factor divided by the max place population specified below. Generally, a
# page will not be demoted more than this value as a percent. 0 means no
# demotion. 
<demotionForNonlocationSpecificQueriesWithALocationSpecificTitle>0.990</>

# Demotion factor for non-location specific queries with a location specific
# summary. Pages which contain a location in their summary which is not in the
# query or the gigabits will be demoted by their population multiplied by this
# factor divided by the max place population specified below. Generally, a
# page will not be demoted more than this value as a percent. 0 means no
# demotion. 
<demotionForNonlocationSpecificQueriesWithALocationSpecificSummary>0.950</>

# Demotion factor for non-location specific queries with a location specific
# dmoz regional category. Pages which contain a location in their dmoz which
# is not in the query or the gigabits will be demoted by their population
# multiplied by this factor divided by the max place population specified
# below. Generally, a page will not be demoted more than this value as a
# percent. 0 means no demotion. 
<demotionForNonlocationSpecificQueriesWithALocationSpecificDmozCategory>0.950</>

# Demote locations that appear in gigabits.
<demoteLocationsThatAppearInGigabits>1</>

# Max place population. Places with a population greater than or equal to this
# will be demoted to the maximum amount given by the factor above as a
# percent. 
<maxValueForNonlocationSpecificQueriesWithLocationSpecificResults>100000</>

# Demotion factor for content type that is non-html. Pages which do not have
# an html content type will be demoted by this factor as a percent. 0 means no
# demotion. A safe range is between 0 and 0.35. 
<demotionForNonhtml>0.000</>

# Demotion factor for content type that is xml. Pages which have an xml
# content type will be demoted by this factor as a percent. 0 means no
# demotion. Any value between 0 and 1 is safe if demotion for non-html is set
# to 0. Otherwise, 0 should probably be used. 
<demotionForXml>0.950</>

# Demotion factor for pages with fewer other pages from same hostname. Pages
# with results from the same host will be demoted by this factor times each
# fewer host than the max value given below, divided by the max value.
# Generally, a page will not be demoted more than this factor as a percent. 0
# means no demotion. A safe range is between 0 and 0.35. 
<demotionForPagesWithOtherPagesFromSameHostname>0.000</>

# Max number of pages from same domain. Pages which have this many or more
# pages from the same domain will not be demoted. 
<maxValueForPagesWithOtherPagesFromSameDomain>12</>

# Initial demotion factor for pages with common topics in dmoz as other
# results. Pages will be penalized by the number of common topics in dmoz
# times this factor divided by the max value given below. Generally, a page
# will not be demoted by more than this factor as a percent. Note: this factor
# is decayed by the factor specified in the parm below, decay for pages with
# common topics in dmoz as other results, as the number of pages with common
# topics in dmoz increases. 0 means no demotion. A safe range is between 0 and
# 0.35. 
<initialDemotionForPagesWithCommonTopicsInDmozAsOtherResults>0.000</>

# Decay factor for pages with common topics in dmoz as other results. The
# initial demotion factor will be decayed by this factor as a percent as the
# number of common topics increase. 0 means no decay. A safe range is between
# 0 and 0.25. 
<decayForPagesWithCommonTopicsInDmozAsOtherResults>0.000</>

# Max number of common topics in dmoz as other results. Pages with a number of
# common topics equal to or greater than this value will be demoted to the
# maximum as given by the initial factor above as a percent. 
<maxValueForPagesWithCommonTopicsInDmozAsOtherResults>32</>

# Demotion factor for pages where dmoz category names contain fewer query
# terms or their synonyms. Pages will be penalized for each query term or
# synonym of a query term less than the max value given below multiplied by
# this factor, divided by the max value. Generally, a page will not be demoted
# more than this value as a percent. 0 means no demotion. A safe range is
# between 0 and 0.3. 
<demotionForPagesWhereDmozCategoryNamesContainQueryTermsOrTheirSynonyms>0.000</>

# Max number of query terms and their synonyms in a page's dmoz category name.
# Pages with a number of query terms or their synonyms in all dmoz category
# names greater than or equal to this value will not be demoted. 
<maxValueForPagesWhereDmozCategoryNamesContainQueryTermsOrTheirSynonyms>10</>

# Demotion factor for pages where dmoz category names contain fewer gigabits.
# Pages will be penalized by the number of gigabits in all dmoz category names
# fewer than the max value given below divided by the max value. Generally, a
# page will not be demoted more than than this factor as a percent. 0 means no
# demotion. A safe range is between 0 and 0.3. 
<demotionForPagesWhereDmozCategoryNamesContainGigabits>0.000</>

# Max number of pages where dmoz category names contain a gigabit. Pages with
# a number of gigabits in all dmoz category names greater than or equal to
# this value will not be demoted. 
<maxValueForPagesWhereDmozCategoryNamesContainGigabits>16</>

# Demotion factor for pages based on datedb date. Pages will be penalized for
# being published earlier than the max date given below. The older the page,
# the more it will be penalized based on the time difference between the
# page's date and the max date, divided by the max date. Generally, a page
# will not be demoted more than this value as a percent. 0 means no demotion.
# A safe range is between 0 and 0.4. 
<demotionForPagesBasedOnDatedbDate>0.000</>

# Pages with a publish date equal to or earlier than this date will be demoted
# to the max (the factor above as a percent). Use this parm in conjunction
# with the max value below to specify the range of dates where demotion
# occurs. If you set this parm near the estimated earliest publish date that
# occurs somewhat frequently, this method can better control the additional
# demotion per publish day. This number is given as seconds since the epoch,
# January 1st, 1970 divided by 1000. 0 means use the epoch. 
<minValueForDemotionBasedOnDatedbDate>631177</>

# Pages with a publish date greater than or equal to this value divided by
# 1000 will not be demoted. Use this parm in conjunction with the min value
# above to specify the range of dates where demotion occurs. This number is
# given as seconds before the current date and time taken from the system
# clock divided by 1000. 0 means use the current time of the current day. 
<maxValueForDemotionBasedOnDatedbDate>0</>

# Demotion factor for proximity of query terms in a document.  The closer
# together terms occur in a document, the higher it will score.0 means no
# demotion. 
<demotionForPagesBasedOnProximity>0.000</>

# Demotion factor for where the query terms occur in the document.  If the
# terms only occur in a menu, a link, or a list, the document will be
# punished.0 means no demotion. 
<demotionForPagesBasedOnQueryTermsSection>0.000</>

# The proportion that the original score affects its rerank position. A factor
# of 1 will maintain the original score, 0 will only use the indexed score to
# break ties.
<weightOfIndexedScoreOnPqr>1.000</>

# Max summary score where no more demotion occurs above. Pages with a summary
# score greater than or equal to this value will not be demoted. 
<maxValueForDemotionForPagesBasedOnProximity>100000</>

# Search result which contains the query terms only as a subphrase of a larger
# phrase will have its score  reduced by this percent.
<demotionForQueryBeingExclusivlyInASubphrase>0.000</>

# Based on the number of inlinks a search results has which are in common with
# another search result.
<demotionBasedOnCommonInlinks>0.500</>

# Allows more results to be gathered in the case of an index having a high
# rate of duplicate results.  Generally expressed as 1.2
<numberOfDocumentCallsMultiplier>1.200</>

# Limit number of linksdb inlinks requested per result.
<maxRealTimeInlinks>10000</>

# If document summary is this percent similar to a document summary above it,
# then remove it from the search results. 100 means only to remove if exactly
# the same. 0 means no summary deduping.
<percentSimilarDedupSummary>90</>

# Sets the number of lines to generate for summary deduping. This is to help
# the deduping process not thorw out valid summaries when normally displayed
# summaries are smaller values. Requires percent similar dedup summary to be
# enabled.
<numberOfLinesToUseInSummaryToDedup>4</>

# Truncating this will miss out on good summaries, but performance will
# increase.
<bytesOfDocToScanForSummaryGeneration>70000</>

# Like above, but used for deciding when to cluster results by topic for the
# news collection.
<percentTopicSimilarDefault>50</>

# Where do we send requests for definitions of search terms. Set to the empty
# string to turn this feature off.
<dictionarySite><![CDATA[http://www.answers.com/]]></>

# Should Gigablast only get one document per IP domain and per domain for
# topic generation?
<ipRestrictionForTopics>0</>

# Should Gigablast remove overlapping topics?
<removeOverlappingTopics>1</>

# How many search results should we scan for related topics per query?
<docsToScanForTopics>300</>

# What is the number of related topics displayed per query? Set to 0 to save
# CPU time.
<numberOfRelatedTopics>11</>

# Related topics with scores below this will be excluded. Scores range from 0%
# to over 100%.
<minTopicsScore>5</>

# How many documents must contain the topic for it to be displayed.
<minTopicDocCount>2</>

# If a document is this percent similar to another document with a higher
# score, then it will not contribute to the topic generation.
<dedupDocPercentForTopics>80</>

# Maximum number of words a topic can have. Affects raw feeds, too.
<maxWordsPerTopic>6</>

# Max chars to sample from each doc for topics.
<topicMaxSampleSize>4096</>

# Max sequential punct chars allowed in a topic. Set to 1 for speed, 5 or more
# for best topics but twice as slow.
<topicMaxPunctLen>1</>

# If enabled while using the XML feed, when Gigablast finds a spelling
# recommendation it will be included in the XML <spell> tag. Default is 0 if
# using an XML feed, 1 otherwise.
<doSpellChecking>1</>

# What is the number of reference pages to generate per query? Set to 0 to
# save CPU time.
<numberOfReferencePagesToGenerate>0</>

# What is the number of reference pages to display per query?
<numberOfReferencePagesToDisplay>0</>

# How many search results should we scan for reference pages per query?
<docsToScanForReferencePages>30</>

# References with page quality below this will be excluded.  (set to 101 to
# disable references while still generating related pages.
<minReferencesQuality>1</>

# References need this many links to results to be included.
<minLinksPerReferences>2</>

# Stop processing referencing pages after hitting this limit.
<maxLinkersToConsiderForReferencesPerPage>500</>

# Use this multiplier to fetch more than the required number of reference
# pages.  fetches N * (this parm) references and displays the top scoring N.
<pageFetchMultiplierForReferences>1.500</>

# A in A * numLinks + B * quality + C * numLinks/totalLinks.
<numberOfLinksCoefficient>0</>

# B in A * numLinks + B * quality + C * numLinks/totalLinks.
<qualityCoefficient>1</>

# C in A * numLinks + B * quality + C * numLinks/totalLinks.
<linkDensityCoefficient>1000</>

# [+|*] in A * numLinks + B * quality [+|*] C * numLinks/totalLinks.
<addOrMultipyQualityTimesLinkDensity>1</>

# maximum allowed value for numReferences parameter
<maximumAllowedValueForNumReferencesParameter>100</>

# maximum allowed value for docsToScanForReferences parameter
<maximumAllowedValueForDocsToScanForReferencesParameter>100</>

# maximum allowed value for maxLinkers parameter
<maximumAllowedValueForMaxLinkersParameter>5000</>

# maximum allowed value for additionalTRFetch parameter
<maximumAllowedValueForAdditionalTRFetch>10.000</>

# number of related pages to generate.
<numberOfRelatedPagesToGenerate>0</>

# number of related pages to display.
<numberOfRelatedPagesToDisplay>0</>

# number of links per reference page to scan for related pages.
<numberOfLinksToScanForRelatedPages>1024</>

# related pages with a quality lower than this will be ignored.
<minRelatedPageQuality>30</>

# related pages with an adjusted score lower than this will be ignored.
<minRelatedPageScore>1</>

# related pages with less than this number of links will be ignored.
<minRelatedPageLinks>2</>

# A in A * numLinks + B * avgLnkrQlty + C * PgQlty + D * numSRPLinks.
<coefficientForNumberOfLinksInRelatedPagesScoreCalculation>10</>

# B in A * numLinks + B * avgLnkrQlty + C * PgQlty + D * numSRPLinks.
<coefficientForAverageLinkerQualityInRelatedPagesScoreCalculation>1</>

# C in A * numLinks + B * avgLnkrQlty + C * PgQlty + D * numSRPLinks
<coefficientForPageQualityInRelatedPagesScoreCalculation>1</>

# D in A * numLinks + B * avgLnkrQlty + C * PgQlty + D * numSRPLinks.
<coefficientForSearchResultLinksInRelatedPagesScoreCalculation>1</>

# What is the maximum number of excerpts displayed in the summary of a related
# page?
<numberOfRelatedPageSummaryExcerpts>1</>

# Highlight query terms in related pages summary.
<highlightQueryTermsInRelatedPagesSummary>0</>

# Truncates a related page title after this many charaters and adds ...
<numberOfCharactersToDisplayInTitleBeforeTruncating>50</>

# Use the search results' links in order to generate related pages.
<useResultsPagesAsReferences>0</>

# Say yes here to make Gigablast check another Gigablast cluster for title rec
# for related pages. Gigablast will use the hosts2.conf file in the working
# directory to tell it what hosts belong to the other cluster.
<getRelatedPagesFromOtherCluster>0</>

# Gigablast will fetch the related pages title record from this collection in
# the other cluster.
<collectionForOtherRelatedPagesCluster><![CDATA[main]]></>

# maximum allowed value for numToGenerate parameter
<maximumAllowedValueForNumToGenerateParameter>100</>

# maximum allowed value for numRPLinksPerDoc parameter
<maximumAllowedValueForNumRPLinksPerDocParameter>5000</>

# maximum allowed value for numSummaryLines parameter
<maximumAllowedValueForNumSummaryLinesParameter>10</>

# Gigablast will import X search results from the external cluster given by
# hosts2.conf and merge those search results into the current set of search
# results. Set to 0 to disable.
<howManyImportedResultsShouldWeInsert>0</>

# The score of all imported results will be multiplied by this number. Since
# results are mostly imported from a large collection they will usually have
# higher scores because of having more link texts or whatever, so tone it down
# a bit to put it on par with the integrating collection.
<importedScoreWeight>0.800</>

# The urls of imported search results must be linked to by at least this many
# documents in the primary collection.
<howManyLinkersMustEachImportedResultHave>3</>

# The number of linkers an imported result has from the base collection is
# multiplied by this weight and then added to the final score. The higher this
# is the more an imported result with a lot of linkers will be boosted.
# Currently, 100 is the max number of linkers permitted.
<numLinkersWeight>50</>

# Gigablast will import X search results from this external collection and
# merge them into the current search results.
<theNameOfTheCollectionToImportFrom><![CDATA[main]]></>

# What is the limit to the total number of returned search results.
<maxSearchResults>1000</>

# What is the limit to the total number of returned search results per query?
<maxSearchResultsPerQuery>100</>

# What is the limit to the total number of returned search results for clients.
<maxSearchResultsForPayingClients>1000</>

# What is the limit to the total number of returned search results per query
# for paying clients? Auto ban must be enabled for this to work.
<maxSearchResultsPerQueryForPayingClients>1000</>

# Max similar results to show when clustering by topic.
<maxSimilarResultsForClusterByTopic>10</>

# number of extra results to get for cluster by topic
<numberOfExtraResultsToGetForClusterByTopic>100</>

# What is the maximum number of characters allowed in titles displayed in the
# search results?
<maxTitleLen>80</>

# Minimum number of in linkers required to consider getting	the title from in
# linkers
<MinimumNumberOfInLinkersRequiredToConsiderGettingTheTitleFromInLinkers>10</>

# Max number of in linkers to consider for getting in linkers titles.
<MaxNumberOfInLinkersToConsider>128</>

# Get docid scoring info?
<getDocidScoringInfo>1</>

# 0 = old compatibility mode, 1 = UTF-8 mode, 2 = fast ASCII mode, 3 = Ascii
# Proximity Summary, 4 = Utf8 Proximity Summary, 5 = Ascii Pre Proximity
# Summary, 6 = Utf8 Pre Proximity Summary:
<summaryMode>0</>

# What is the maximum number of characters displayed in a summary for a search
# result?
<maxSummaryLen>512</>

# What is the maximum number of excerpts displayed in the summary of a search
# result?
<maxSummaryExcerpts>4</>

# What is the maximum number of characters allowed per summary excerpt?
<maxSummaryExcerptLength>300</>

# What is the default number of summary excerpts displayed per search result?
<defaultNumberOfSummaryExcerpts>3</>

# <br> tags are inserted to keep the number of chars in the summary per line
# at or below this width. Strings without spaces that exceed this width are
# not split.
<maxSummaryLineWidth>80</>

# Maximum number of characters to allow in between search terms.
<ProxSummaryCarverRadius>256</>

# Front html tag used for highlightig query terms in the summaries displated
# in the search results.
<frontHighlightTag><![CDATA[&lt;b style=&#34;color:black;background-color:&#035;ffff66&#34;&gt;]]></>

# Front html tag used for highlightig query terms in the summaries displated
# in the search results.
<backHighlightTag><![CDATA[&lt;/b&gt;]]></>

# Query expansion will include word stems and synonyms in its search results.
<doQueryExpansion>1</>

# Can Gigablast make titles from the document content? Used mostly for the
# news collection where the title tags are not very reliable.
<considerTitlesFromBody>0</>

# If enabled, results in dmoz will display their categories on the results
# page.
<displayDmozCategoriesInResults>1</>

# If enabled, results in dmoz will display their indirect categories on the
# results page.
<displayIndirectDmozCategoriesInResults>0</>

# If enabled, a link will appear next to each category on each result allowing
# the user to perform their query on that entire category.
<displaySearchCategoryLinkToQueryCategoryOfResult>0</>

# Yes to use DMOZ given title when a page is untitled but is in DMOZ.
<useDmozForUntitled>1</>

# Yes to always show DMOZ summaries with search results that are in DMOZ.
<showDmozSummaries>1</>

# Yes to display the Adult category in the Top category
<showAdultCategoryOnTop>0</>

# Display the indexed date along with results.
<displayIndexedDate>1</>

# Display the last modified date along with results.
<displayLastModifiedDate>1</>

# Display the published (datedb) date along with results.
<displayPublishedDate>0</>

# The [cached] link on results pages loads click n scroll.
<enableClicknScroll>0</>

# Enable/disable the use of a remote account verification for Data Feed
# Customers.
<useDataFeedAccountServer>0</>

# The ip address of the Gigablast data feed server to retrieve customer
# account information from.
<dataFeedServerIp>127.0.0.1</>

# The port of the Gigablast data feed server to retrieve customer account
# information from.
<dataFeedServerPort>8040</>

# The collection on the Gigablast data feed server to retrieve customer
# account information from.
<dataFeedServerCollection><![CDATA[customers]]></>

# Hostname that will default to this collection. Blank for none or default
# collection.
<collectionHostname><![CDATA[]]></>

# Hostname that will default to this collection. Blank for none or default
# collection.
<collectionHostname1><![CDATA[]]></>

# Hostname that will default to this collection. Blank for none or default
# collection.
<collectionHostname2><![CDATA[]]></>

# Passwords allowed to perform searches on this collection. If no passwords
# are specified, then anyone can search it.
# Use <searchPassword> tag.

# These IPs are not allowed to search this collection or use add url. Useful
# to keep out miscreants. Use zero for the last number of the IP to ban an
# entire IP domain.
# Use <bannedIp> tag.

# Only these IPs are allowed to search the collection and use the add url
# facilities. If you'd like to make your collection publically searchable then
# do not add any IPs here.Use zero for the last number of the IP to restrict
# to an entire IP domain, i.e. 1.2.3.0.
# Use <searchIp> tag.

# Browsers coming from these IPs are deemed to be spam assassins and have
# access to a subset of the controls to ban and remove domains and IPs from
# the index.
# Use <assassinIp> tag.

# Passwords allowed to edit this collection record. First password can only be
# deleted by the master administrator. If no password of Admin IP is given at
# time of creation then the default password of 'footbar23' will be assigned.
# Use <adminPassword> tag.

# If someone connects from one of these IPs and provides a password from the
# table above then they will have full administrative priviledges for this
# collection. If you specified no Admin Passwords above then they need only
# connect from an IP in this table to get the privledges. 
# Use <adminIp> tag.

# Before downloading the contents of a URL, Gigablast first chains down this
# list of <a href=/overview.html#regex>expressions</a>, starting with
# expression #1.  This table is also consulted for every outlink added to
# spiderdb. When it finds a regular expression that matches that URL, it
# assigns the corresponding <a href=/overview.html#spiderfreq>spider
# frequency</a>, <a href=/overview.html#spiderpriority>spider priority</a> to
# that URL. If no expression is matched, then the <i>default</i> line is
# used.<br><br>
<filterExpression><![CDATA[isdocidbased]]></>
<filterExpression><![CDATA[$.css]]></>
<filterExpression><![CDATA[$.mpeg]]></>
<filterExpression><![CDATA[$.mpg]]></>
<filterExpression><![CDATA[$.mp3]]></>
<filterExpression><![CDATA[$.wmv]]></>
<filterExpression><![CDATA[.css?]]></>
<filterExpression><![CDATA[$.jpg]]></>
<filterExpression><![CDATA[$.JPG]]></>
<filterExpression><![CDATA[$.gif]]></>
<filterExpression><![CDATA[$.ico]]></>
<filterExpression><![CDATA[/print/]]></>
<filterExpression><![CDATA[errorcount&gt;=3 &amp;&amp; hastmperror]]></>
<filterExpression><![CDATA[errorcount&gt;=1 &amp;&amp; hastmperror]]></>
<filterExpression><![CDATA[isaddurl]]></>
<filterExpression><![CDATA[hopcount==0 &amp;&amp; iswww &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount==0 &amp;&amp; iswww]]></>
<filterExpression><![CDATA[hopcount==0 &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount==0]]></>
<filterExpression><![CDATA[hopcount==1 &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount==1]]></>
<filterExpression><![CDATA[hopcount==2 &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount==2]]></>
<filterExpression><![CDATA[hopcount&gt;=3 &amp;&amp; isnew]]></>
<filterExpression><![CDATA[hopcount&gt;=3]]></>
<filterExpression><![CDATA[isnew]]></>
<filterExpression><![CDATA[default]]></>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>1</>
<spidersEnabled>0</>
<spidersEnabled>1</>
<spidersEnabled>0</>
<spidersEnabled>1</>
<spidersEnabled>0</>
<spidersEnabled>1</>
<spidersEnabled>0</>
<spidersEnabled>1</>
<spidersEnabled>0</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>1.000</>
<filterFrequency>1.000</>
<filterFrequency>1.000</>
<filterFrequency>0.000</>
<filterFrequency>7.000</>
<filterFrequency>0.000</>
<filterFrequency>10.000</>
<filterFrequency>0.000</>
<filterFrequency>20.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>0.000</>
<filterFrequency>30.000</>
<filterFrequency>30.000</>

# Do not allow more than this many outstanding spiders for all urls in this
# priority.
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>4</>
<maxSpidersPerRule>2</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>2</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>1</>
<maxSpidersPerRule>99</>
<maxSpidersPerRule>99</>

# Allow this many spiders per IP.
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>
<maxSpidersPerIp>1</>

# Wait at least this long before downloading urls from the same IP address.
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<spiderIpWait>1000</>
<filterPriority>80</>
<filterPriority>-3</>
<filterPriority>-3</>
<filterPriority>-3</>
<filterPriority>-3</>
<filterPriority>-3</>
<filterPriority>-3</>
<filterPriority>-3</>
<filterPriority>0</>
<filterPriority>-3</>
<filterPriority>-3</>
<filterPriority>-3</>
<filterPriority>3</>
<filterPriority>45</>
<filterPriority>85</>
<filterPriority>50</>
<filterPriority>48</>
<filterPriority>49</>
<filterPriority>47</>
<filterPriority>40</>
<filterPriority>39</>
<filterPriority>30</>
<filterPriority>29</>
<filterPriority>20</>
<filterPriority>19</>
<filterPriority>1</>
<filterPriority>0</>
